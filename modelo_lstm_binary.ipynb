{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "Este cuaderno cubre el proceso de desarrollo de un modelo de NLP (LSTM) para predecir el sentimiento de los tweets relacionados con el cambio climático. Cubrirá cuatro fases principales:\n",
    "\n",
    "- Modelado y entrenamiento: donde se construirá el modelo.\n",
    "- Optimización: donde se registrarán y compararán las métricas adquiridas para diferentes combinaciones de las fases anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de las funciones y librerias pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_tweets_lstm_rnn.csv')[['RawContent', 'Sentiment']]\n",
    "df['RawContent']  = df['RawContent'].apply(lambda x: [i.replace('\\'', '').replace(']', '').replace('[', '') for i in x.split(', ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan los tweets neutrales\n",
    "df = df[df['Sentiment'] != 1]\n",
    "df.loc[df['Sentiment'] == 2, 'Sentiment'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts  = df['RawContent'] \n",
    "labels = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizacion\n",
    "\n",
    "word2vec_model = Word2Vec(texts, vector_size=100, window=5, min_count=2)\n",
    "word2vec_model.train(texts, total_examples=len(texts), epochs=5)\n",
    "\n",
    "word_indices = {word: (index+1) for word, index in word2vec_model.wv.key_to_index.items()}\n",
    "word_vectors = np.vstack([np.zeros((1, word2vec_model.wv.vectors.shape[1])), word2vec_model.wv.vectors])\n",
    "\n",
    "# Aplicar el diccionario de palabras de word2vec a los textos\n",
    "sequences = []\n",
    "for sentence in texts:\n",
    "    sentence_indices = [0 if word not in word_indices else word_indices[word] for word in sentence]\n",
    "    sequences.append(sentence_indices)\n",
    "\n",
    "# Añadir padding a las secuencias para que todas tengan la misma longitud\n",
    "max_length = max(len(sequence) for sequence in sequences)+1\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding de los labels\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos de entrenamiento y test\n",
    "\n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['learning_rate', 'number_of_neurons', 'loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.05, 0.01, 0.005, 0.001]\n",
    "neurons = [32,64,128,256]\n",
    "\n",
    "for n in neurons:\n",
    "    for lr in learning_rates:\n",
    "        \n",
    "        print(f'Learning rate: {lr}, Number of neurons: {n}')\n",
    "        # Definimos el modelo de red neuronal. Será un modelo secuencial, es decir, \n",
    "        # una pila de capas de neuronas que reciben la salida de la capa anterior como entrada.\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Embedding(\n",
    "            input_dim=len(word_indices)+1, \n",
    "            output_dim=100, \n",
    "            embeddings_initializer=Constant(word_vectors), \n",
    "            trainable=False)\n",
    "        )\n",
    "        model.add(LSTM(n, return_sequences=True))\n",
    "        model.add(LSTM(n*2, return_sequences=True))\n",
    "        model.add(LSTM(n))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=lr), \n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        model.fit(tweets_train, labels_train, \n",
    "                  batch_size=64, epochs=100, \n",
    "                  verbose=1, \n",
    "                  validation_data=(tweets_test, labels_test), callbacks=[early_stop])\n",
    "        \n",
    "        loss, accuract = model.evaluate(tweets_test, labels_test)\n",
    "        \n",
    "        new_row = pd.DataFrame({'learning_rate':[lr], 'number_of_neurons':[n], \n",
    "                                'loss':[loss], 'accuracy':[accuract]})\n",
    "        \n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "        results.to_csv('results_lstm_binary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
